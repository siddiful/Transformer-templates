# -*- coding: utf-8 -*-
"""Encoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JHwnAUEGFQDcmvS0vzqbxPFuzU2oWM3z
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import dataset
import numpy as np
import matplotlib.pyplot as plt

class MultiHeadAttention(nn.Module):
  def __init__(self, d_model, d_k, n_heads):
    super().__init__()
    self.d_k = d_k
    self.n_heads = n_heads
    self.query = nn.Linear(d_model, d_k*n_heads)
    self.key = nn.Linear(d_model, d_k*n_heads)
    self.value = nn.Linear(d_model, d_k*n_heads)
    self.fc = nn.Linear(d_k*n_heads, d_model)

  def forward(self, q, k, v, mask=None):
    q = self.query(q) # (B, T, d_k*n_heads)
    k = self.query(k) # (B, T, d_k*n_heads)
    v = self.query(v) # (B, T, d_k*n_heads)

    B = q.shape[0]
    T = q.shape[1]

    q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2) # (B, n_heads, T, d_k)
    k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2) # (B, n_heads, T, d_k)
    v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2) # (B, n_heads, T, d_k)

    dot_prod = q @ k.transpose(-2, -1) / math.sqrt(self.d_k) # (B, n_heads, T, T)
    if mask is not None:
      dot_prod = dot_prod.masked_fill(mask[:, None, None, :]==0, float('-inf'))
    attention_wts = F.softmax(dot_prod, dim=-1) # (B, n_heads, T, T)
    A = attention_wts @ v # (B, n_heads, T, d_k)
    A = A.transpose(1, 2)
    A = A.contiguous().view(B, T, self.d_k * self.n_heads) # (B, T, n_heads*d_k)

    return self.fc(A)

class TransformerBlock(nn.Module):
  def __init__(self, d_model, d_k, n_heads, dropout_prob = 0.1):
    super().__init__()
    self.mha = MultiHeadAttention(d_model, d_k, n_heads)
    self.ln1 = nn.LayerNorm(d_model)
    self.ln2 = nn.LayerNorm(d_model)
    self.ffn = nn.Sequential(
        nn.Linear(d_model, 4*d_model),
        nn.GELU(),
        nn.Linear(4*d_model, d_model),
        nn.Dropout(dropout_prob),
    )
    self.dropout = nn.Dropout(p=dropout_prob)

  def forward(self, x, mask=None):
    x = self.ln1(x + self.mha(x, x, x, mask))
    x = self.ln2(x + self.ffn(x))
    x = self.dropout(x)
    return x

class PositionalEncoding(nn.Module):
  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):
    super().__init__()
    self.dropout = nn.Dropout(p=dropout_prob)

    position = torch.arange(max_len).unsqueeze(1)
    exp_term = torch.arange(0, d_model, 2)
    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))
    pe = torch.zeros(1, max_len, d_model)
    pe[0, :, 0::2] = torch.sin(position * div_term)
    pe[0, :, 1::2] = torch.cos(position * div_term)
    self.register_buffer('pe', pe)

  def forward(self, x):
    x = x + self.pe[:, :x.size(1), :]
    return self.dropout(x)

class Encoder(nn.Module):
  def __init__(self, d_model, d_k, n_heads, max_len, vocab_size , n_layers, n_classes, dropout_prob):
    super().__init__()
    self.embedding = nn.Embedding(vocab_size, d_model)
    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)
    self.transformer_blocks = nn.Sequential(*[TransformerBlock(d_model, d_k, n_heads, dropout_prob) for _ in range(n_layers)])
    self.ln = nn.LayerNorm(d_model)
    self.fc = nn.Linear(d_model, n_classes)

  def forward(self, x, mask=None):
    x = self.embedding(x) #(B, T, d_model)
    x = self.pos_encoding(x) #(B, T, d_model)

    for block in self.transformer_blocks:
      x = block(x, mask)

    x = x[:, 0, :] #(B, d_model)
    x = self.ln(x) #(B, d_model)
    x = self.fc(x) #(B, n_classes)

    return x

model = Encoder(64, 16, 4, 1024, 20000, 4, 5, 0.1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

x = np.random.randint(0, 20000, (8, 512))
x_t = torch.tensor(x).to(device)

mask = np.ones((8, 512))
mask[:, 256:] = 0
mask_t = torch.tensor(mask).to(device)

m = model(x_t, mask_t)

m

!pip install transformers datasets

from transformers import AutoTokenizer, DataCollatorWithPadding
from datasets import load_dataset

checkpoints = "distilbert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(checkpoints)

raw_datasets = load_dataset("glue", "sst2")
raw_datasets

def tokenize_fn(batch):
  return tokenizer(batch['sentence'], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True, remove_columns=['sentence', 'idx'])
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')
tokenized_datasets

data_collator

tokenizer.model_max_length

model = Encoder(
    d_model=64,
    d_k=16,
    n_heads=4,
    max_len=tokenizer.model_max_length,
    vocab_size=tokenizer.vocab_size,
    n_layers=6,
    n_classes=2,
    dropout_prob=0.1,
)

model.to(device)

from torch.utils.data import DataLoader

train_loader = DataLoader(
    tokenized_datasets['train'],
    batch_size=32,
    shuffle=True,
    collate_fn=data_collator,
)

valid_loader = DataLoader(
    tokenized_datasets['validation'],
    batch_size=32,
    collate_fn=data_collator,
)

for batch in train_loader:
  for k, v in batch.items():
    print(k, v.shape)

loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.AdamW(model.parameters())

from datetime import datetime

def train(model, train_loader, valid_loader, loss_fn, optimiser, n_epochs):
  train_losses = np.zeros(n_epochs)
  valid_losses = np.zeros(n_epochs)

  for epoch in range(n_epochs):
    model.train()
    t0 = datetime.now()
    train_loss=0
    n_train=0
    for batch in train_loader:
      batch = {k: v.to(device) for k, v in batch.items()}
      optimiser.zero_grad()
      outputs = model(batch['input_ids'], batch['attention_mask'])
      loss = loss_fn(outputs, batch['labels'])
      loss.backward()
      optimiser.step()
      train_loss += loss.item()*batch['input_ids'].size(0)
      n_train += batch['input_ids'].size(0)

    train_loss = train_loss/n_train
    model.eval()
    eval_loss=0
    n_eval=0
    for batch in valid_loader:
      batch = {k: v.to(device) for k, v in batch.items()}
      outputs = model(batch['input_ids'], batch['attention_mask'])
      loss = loss_fn(outputs, batch['labels'])
      eval_loss += loss.item()*batch['input_ids'].size(0)
      n_eval += batch['input_ids'].size(0)

    eval_loss = eval_loss/n_eval
    train_losses[epoch]=train_loss
    valid_losses[epoch]=eval_loss
    dt = datetime.now() - t0
    print(f'Train loss: {train_loss}, Eval loss: {eval_loss}, Time: {dt}')
  return train_losses, valid_losses

train(model, train_loader, valid_loader, loss_fn, optimiser, n_epochs=6)

model.eval()
n_correct=0.
n_total=0.

for batch in train_loader:
  batch = {k: v.to(device) for k, v in batch.items()}
  outputs = model(batch['input_ids'], batch['attention_mask'])
  _, predictions = torch.max(outputs, dim=1)
  n_correct += (predictions == batch['labels']).sum().item()
  n_total += batch['labels'].size(0)

print(f'Train Accuracy: {n_correct/n_total}')

n_correct=0.
n_total=0.

for batch in valid_loader:
  batch = {k: v.to(device) for k, v in batch.items()}
  outputs = model(batch['input_ids'], batch['attention_mask'])
  _, predictions = torch.max(outputs, dim=1)
  n_correct += (predictions == batch['labels']).sum().item()
  n_total += batch['labels'].size(0)

print(f'Validation Accuracy: {n_correct/n_total}')