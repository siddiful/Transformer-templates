# -*- coding: utf-8 -*-
"""Decoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rETUMEUrDk-lxSmyqkSgD_nkKE_IeUZE
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import dataset
import numpy as np
import matplotlib.pyplot as plt

class CausalSelfAttention(nn.Module):
  def __init__(self, d_model, d_k, n_heads, max_len):
    super().__init__()
    self.d_k = d_k
    self.n_heads = n_heads
    self.query = nn.Linear(d_model, d_k*n_heads)
    self.key = nn.Linear(d_model, d_k*n_heads)
    self.value = nn.Linear(d_model, d_k*n_heads)
    self.fc = nn.Linear(d_k*n_heads, d_model)
    cm = torch.tril(torch.ones(max_len, max_len))
    self.register_buffer('causal_mask', cm.view(1, 1, max_len, max_len))

  def forward(self, q, k, v, pad_mask=None):
    q = self.query(q) # (B, T, d_k*n_heads)
    k = self.query(k) # (B, T, d_k*n_heads)
    v = self.query(v) # (B, T, d_k*n_heads)

    B = q.shape[0]
    T = q.shape[1]

    q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2) # (B, n_heads, T, d_k)
    k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2) # (B, n_heads, T, d_k)
    v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2) # (B, n_heads, T, d_k)

    dot_prod = q @ k.transpose(-2, -1) / math.sqrt(self.d_k) # (B, n_heads, T, T)
    if pad_mask is not None:
      dot_prod = dot_prod.masked_fill(pad_mask[:, None, None, :]==0, float('-inf'))
    dot_prod = dot_prod.masked_fill(self.causal_mask[:, :, :T, :T]==0, float('-inf'))
    attention_wts = F.softmax(dot_prod, dim=-1) # (B, n_heads, T, T)
    A = attention_wts @ v # (B, n_heads, T, d_k)
    A = A.transpose(1, 2)
    A = A.contiguous().view(B, T, self.d_k * self.n_heads) # (B, T, n_heads*d_k)

    return self.fc(A)

class TransformerBlock(nn.Module):
  def __init__(self, d_model, d_k, n_heads, max_len, dropout_prob = 0.1):
    super().__init__()
    self.mha = CausalSelfAttention(d_model, d_k, n_heads, max_len)
    self.ln1 = nn.LayerNorm(d_model)
    self.ln2 = nn.LayerNorm(d_model)
    self.ffn = nn.Sequential(
        nn.Linear(d_model, 4*d_model),
        nn.GELU(),
        nn.Linear(4*d_model, d_model),
        nn.Dropout(dropout_prob),
    )
    self.dropout = nn.Dropout(p=dropout_prob)

  def forward(self, x, pad_mask=None):
    x = self.ln1(x + self.mha(x, x, x, pad_mask))
    x = self.ln2(x + self.ffn(x))
    x = self.dropout(x)
    return x

class PositionalEncoding(nn.Module):
  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):
    super().__init__()
    self.dropout = nn.Dropout(p=dropout_prob)

    position = torch.arange(max_len).unsqueeze(1)
    exp_term = torch.arange(0, d_model, 2)
    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))
    pe = torch.zeros(1, max_len, d_model)
    pe[0, :, 0::2] = torch.sin(position * div_term)
    pe[0, :, 1::2] = torch.cos(position * div_term)
    self.register_buffer('pe', pe)

  def forward(self, x):
    x = x + self.pe[:, :x.size(1), :]
    return self.dropout(x)

class Decoder(nn.Module):
  def __init__(self, d_model, d_k, n_heads, max_len, vocab_size , n_layers, dropout_prob):
    super().__init__()
    self.embedding = nn.Embedding(vocab_size, d_model)
    self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)
    self.transformer_blocks = nn.Sequential(*[TransformerBlock(d_model, d_k, n_heads, max_len, dropout_prob) for _ in range(n_layers)])
    self.ln = nn.LayerNorm(d_model)
    self.fc = nn.Linear(d_model, vocab_size)

  def forward(self, x, pad_mask=None):
    x = self.embedding(x) #(B, T, d_model)
    x = self.pos_encoding(x) #(B, T, d_model)

    for block in self.transformer_blocks:
      x = block(x, pad_mask)

    x = self.ln(x) #(B, d_model)
    x = self.fc(x) #(B, vocab_size)

    return x

model = Decoder(64, 16, 4, 1024, 20000, 4, 0.1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

x = np.random.randint(0, 20000, (8, 512))
x_t = torch.tensor(x).to(device)

mask = np.ones((8, 512))
mask[:, 256:] = 0
mask_t = torch.tensor(mask).to(device)

m = model(x_t, mask_t)

m.shape, m

!pip install transformers datasets

from transformers import AutoTokenizer, DataCollatorWithPadding
from datasets import load_dataset

checkpoints = "distilbert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(checkpoints)

raw_datasets = load_dataset("glue", "sst2")
raw_datasets

def tokenize_fn(batch):
  return tokenizer(batch['sentence'], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True, remove_columns=raw_datasets['train'].column_names)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

tokenized_datasets

data_collator

tokenizer.model_max_length

tokenizer.pad_token_id

tokenizer.vocab_size

model = Decoder(
    d_model=64,
    d_k=16,
    n_heads=4,
    max_len=tokenizer.model_max_length,
    vocab_size=tokenizer.vocab_size,
    n_layers=6,
    dropout_prob=0.1,
)

model.to(device)

from torch.utils.data import DataLoader

train_loader = DataLoader(
    tokenized_datasets['train'],
    batch_size=32,
    shuffle=True,
    collate_fn=data_collator,
)

valid_loader = DataLoader(
    tokenized_datasets['validation'],
    batch_size=32,
    collate_fn=data_collator,
)

for batch in train_loader:
  for k, v in batch.items():
    print(k, v.shape)

loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
optimiser = torch.optim.AdamW(model.parameters())

from datetime import datetime

def train(model, train_loader, loss_fn, optimiser, n_epochs):
  train_losses = np.zeros(n_epochs)

  for epoch in range(n_epochs):
    model.train()
    t0 = datetime.now()
    train_loss=0
    n_train=0
    for batch in train_loader:
      batch = {k: v.to(device) for k, v in batch.items()}
      optimiser.zero_grad()
      targets = batch['input_ids'].clone().detach()
      targets = torch.roll(targets, shifts=-1, dims=1)
      targets[:, -1] = tokenizer.pad_token_id
      outputs = model(batch['input_ids'], batch['attention_mask'])
      loss = loss_fn(outputs.transpose(2, 1), targets)
      loss.backward()
      optimiser.step()
      train_loss += loss.item()*batch['input_ids'].size(0)
      n_train += batch['input_ids'].size(0)

    train_loss = train_loss/n_train
    train_losses[epoch]=train_loss
    dt = datetime.now() - t0
    print(f'Train loss: {train_loss}, Time: {dt}')
  return train_losses

train(model, train_loader, loss_fn, optimiser, n_epochs=15)